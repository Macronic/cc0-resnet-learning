{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "137b5474-34a9-4580-9da1-2845a38ddc10",
   "metadata": {},
   "source": [
    "# J. Paul Getty Museum Collection Download\n",
    "The museum is sharing an art collection dataset with some CC0 license, what we can use to train our CC0 ResNet backbone for other projects!\n",
    "Their docs page: https://data.getty.edu/museum/collection/docs/ contains documentation how to download the images from their API.\n",
    "We can use the image as long as it has a valid '\"id\": \"https://creativecommons.org/publicdomain/zero/1.0/\"' field in its data, what we will filter on. The dataset may contain up to 80000 images, and we will need a lot of that to train a ResNet backbone that would have transferable knowledge.\n",
    "\n",
    "Note that their API is just awful, the SPARQL server randomly time-outs but we do need the ids, so I used https://data.getty.edu/museum/collection/sparql-ui and filled 'metadata/GET_crawler_ids.txt' with them by hand, using this query:\n",
    "\n",
    "```\n",
    "    PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "    PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    SELECT DISTINCT ?sub  WHERE {\n",
    "      ?sub ?pred ?obj .\n",
    "      FILTER (regex(str(?sub), \"collection/object\") && ?ctt = 6).\n",
    "      bind(((strlen(str(?sub)) - strlen(replace(str(?sub), \"/\", \"\"))) / strlen(\"/\")) as ?ctt)\n",
    "   } LIMIT 10;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969e9ce-eba1-4354-b7d9-b7142f855395",
   "metadata": {},
   "source": [
    "### Constants and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05091c7-2b58-4635-a6c8-a36c56cfd235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from atomicwrites import atomic_write\n",
    "\n",
    "BASE_METADATA_URL = 'https://data.getty.edu/museum/collection/object'\n",
    "COLLECTION_URL = 'https://data.getty.edu/museum/collection'\n",
    "METADATA_DIRECTORY = 'metadata'\n",
    "DATASET_NAME = 'get'\n",
    "RAW_METADATA_DIRECTORY = os.path.join(METADATA_DIRECTORY, 'raw')\n",
    "RAW_IMAGE_DIRECTORY = os.path.join('dataset', 'raw')\n",
    "DATASET_IMAGES_LIMIT = 70000\n",
    "DATASET_IDS_LIMIT = 100000\n",
    "DOWNLOADING_SAVE_PERIOD = 5\n",
    "BASE_CRAWL_FILE = os.path.join(METADATA_DIRECTORY, 'GET_crawler_ids.txt')\n",
    "ACCEPT = \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\"\n",
    "ACCEPT_ENCODING = \"gzip, deflate, br\"\n",
    "ACCEPT_LANGUAGE = \"en-US,en;q=0.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d35dc6e-77a9-44ae-8a9e-f6a9a6e307f5",
   "metadata": {},
   "source": [
    "### Preparing directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70fb961-5fb5-402b-96c0-2b7407b685f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(RAW_IMAGE_DIRECTORY):\n",
    "    os.makedirs(RAW_IMAGE_DIRECTORY)\n",
    "\n",
    "if not os.path.exists(RAW_METADATA_DIRECTORY):\n",
    "    os.makedirs(RAW_METADATA_DIRECTORY)\n",
    "    \n",
    "if not os.path.exists(METADATA_DIRECTORY):\n",
    "    os.makedirs(METADATA_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e9e45-cb39-45cd-86d2-29d39c614754",
   "metadata": {},
   "source": [
    "### Loading started ids for crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01ca34c-a20a-48fa-b7ac-26fb7bcf8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_CRAWL_FILE, 'r') as f:\n",
    "    starting_crawl_queue = [id.replace('<', '').replace('>', '').strip() for id in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7690eb-1d4b-46d9-aa04-59723ad08b93",
   "metadata": {},
   "source": [
    "### Checking existing metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db10c1af-4b81-4b96-9dcc-8c7f7221b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_metadata = {}\n",
    "raw_metadata = {}\n",
    "\n",
    "BASIC_METADATA_PATH = os.path.join(METADATA_DIRECTORY, f'{DATASET_NAME}.json')\n",
    "RAW_METADATA_PATH = os.path.join(METADATA_DIRECTORY, f'{DATASET_NAME}-raw.json')\n",
    "CRAWL_QUEUE_PATH = os.path.join(METADATA_DIRECTORY, f'{DATASET_NAME}-ids.json')\n",
    "\n",
    "if os.path.exists(BASIC_METADATA_PATH):\n",
    "    with open(BASIC_METADATA_PATH, 'r') as f:\n",
    "        existing_metadata = json.load(f)\n",
    "\n",
    "if os.path.exists(RAW_METADATA_PATH):\n",
    "    with open(RAW_METADATA_PATH, 'r') as f:\n",
    "        raw_metadata = json.load(f)\n",
    "\n",
    "if os.path.exists(CRAWL_QUEUE_PATH):\n",
    "    with open(CRAWL_QUEUE_PATH, 'r') as f:\n",
    "        starting_crawl_queue = json.load(f)\n",
    "\n",
    "def save_metadata():\n",
    "    with atomic_write(BASIC_METADATA_PATH, overwrite=True) as f:\n",
    "        json.dump(existing_metadata, f)\n",
    "\n",
    "def save_raw_metadata():\n",
    "    with atomic_write(RAW_METADATA_PATH, overwrite=True) as f:\n",
    "        json.dump(raw_metadata, f)\n",
    "\n",
    "def save_crawl_queue():\n",
    "    with atomic_write(CRAWL_QUEUE_PATH, overwrite=True) as f:\n",
    "        json.dump(crawl_queue, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f5ff1a-1c96-471f-9588-2750b145e14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_queue = []\n",
    "all_ids = set()\n",
    "\n",
    "for id in starting_crawl_queue:\n",
    "    all_ids.add(id.split('/')[6].strip())\n",
    "    if id not in raw_metadata:\n",
    "        crawl_queue.append(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eadca95-7368-4f5d-8d78-fa2969daa19c",
   "metadata": {},
   "source": [
    "### Crawl through the site, readying raw metadata and further IDs to crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ecd766a-d4e3-431e-9fb3-10de57646eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(d):\n",
    "    for i in getattr(d, 'values', lambda :d)():\n",
    "        if isinstance(i, str):\n",
    "            yield i\n",
    "        elif i is not None and isinstance(i, (dict, list)):\n",
    "            yield from flatten(i)\n",
    "\n",
    "def fill_crawl_queue(result):\n",
    "    possible_ids = flatten(result)\n",
    "    for value in possible_ids:\n",
    "        if COLLECTION_URL in value:\n",
    "            trimmed_value = value.split('/')[6].strip()\n",
    "            if trimmed_value not in raw_metadata and trimmed_value not in all_ids:\n",
    "                all_ids.add(trimmed_value)\n",
    "                crawl_queue.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f204b-1540-4df9-8c34-067e63a56e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c2e94bc6b44e29ad424f94740e635c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0 \n",
    "\n",
    "with tqdm(total=DATASET_IDS_LIMIT) as pbar:\n",
    "    while len(crawl_queue) > 0 and DATASET_IDS_LIMIT > len(raw_metadata):\n",
    "        url = crawl_queue[0].strip()\n",
    "        crawl_queue = crawl_queue[1:]\n",
    "        data = requests.get(url, headers={'Accept': ACCEPT, \"Accept-Encoding\": ACCEPT_ENCODING, \"Accept-Language\": ACCEPT_LANGUAGE}).json()\n",
    "        if BASE_METADATA_URL in url:\n",
    "            current_id = url.split('/')[6].strip()\n",
    "            raw_metadata[current_id] = data\n",
    "        fill_crawl_queue(data)\n",
    "        if i % DOWNLOADING_SAVE_PERIOD == 0:\n",
    "            save_raw_metadata()\n",
    "            save_crawl_queue()\n",
    "        i += 1\n",
    "        pbar.update(len(raw_metadata) - pbar.n)\n",
    "        pbar.set_description(f'queue_size={len(crawl_queue)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ad9f9-fff1-4328-892a-85250654e39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e9595-305e-45c0-8bbd-dcf7d193dcfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d81c6a-9445-4e62-b968-3f980112a973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
