{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d4f4620-fcd6-4d20-818d-170612cb94e4",
   "metadata": {},
   "source": [
    "# ResNet-50 Transfer Learning Testing\n",
    "In this notebook, we'll be loading the last trained checkpoint and comparing it to ResNet-50 trained on ImageNet, using Flower Classification CC BY 4.0 dataset, [available here](https://data.mendeley.com/datasets/738sdjm6h9/1).\n",
    "\n",
    "We try to use the same hyperparameters as [this notebook](https://github.com/ovh/ai-training-examples/blob/main/notebooks/computer-vision/image-classification/tensorflow/resnet50/notebook-resnet-transfer-learning-image-classification.ipynb), so that we're able to compare it to a ResNet-50 trained on ImageNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dfe3bc-cf35-4601-a5a7-f1a799aa79cb",
   "metadata": {},
   "source": [
    "### Constants and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3451223-aadc-4855-830d-05093b9b5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "import tarfile\n",
    "from tqdm.notebook import tqdm\n",
    "from lightning.pytorch import Trainer, seed_everything, LightningModule\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import utils\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "CHECKPOINTS_DIRECTORY = 'checkpoints'\n",
    "FLOWER_CHECKPOINTS_DIRECTORY = 'flowers-checkpoints'\n",
    "FLOWER_DATASET_DIRECTORY = 'flowers'\n",
    "FLOWER_DATASET_ARCHIVE_FILENAME = 'flower_photos.tgz'\n",
    "\n",
    "# Notebook and Keras Adam default values\n",
    "USE_RANDOM_MODEL = False\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "EPSILON = 1e-7\n",
    "\n",
    "TRAIN_DATASET_RATIO = 0.7\n",
    "VAL_DATASET_RATIO = 0.2\n",
    "TEST_DATASET_RATIO = 0.1\n",
    "\n",
    "FLOWER_CLASSIFICATION_URL = 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95ae65-18a2-40ce-b1b0-170a303f960f",
   "metadata": {},
   "source": [
    "### Download Flower Classification dataset if it isn't downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e19232d-8a7c-4ff4-92e1-cb05a51a7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(FLOWER_DATASET_DIRECTORY):\n",
    "    os.makedirs(FLOWER_DATASET_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46cdc421-0f84-49ad-88c1-6ef8b466a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_downloaded = False\n",
    "is_unpacked = False\n",
    "\n",
    "def download_file(url, path):\n",
    "    image_data = requests.get(url, stream=True)\n",
    "    if image_data.status_code == 200:\n",
    "        with open(path, 'wb') as f:\n",
    "            for chunk in image_data.iter_content(2048):\n",
    "                f.write(chunk)\n",
    "\n",
    "def extract_flower_archive(archive_path, target_path):\n",
    "    tar = tarfile.open(archive_path, 'r')\n",
    "    for file in tar.getmembers():\n",
    "        if file.isdir() or '.jpg' in file.name:\n",
    "            tar.extract(file, target_path)\n",
    "\n",
    "flower_directory_listing = os.listdir(FLOWER_DATASET_DIRECTORY)\n",
    "if len(flower_directory_listing) != 0:\n",
    "    if 'flower_photos.tgz' in flower_directory_listing:\n",
    "        is_downloaded = True\n",
    "    if 'flower_photos' in flower_directory_listing:\n",
    "        flower_photos_listing = os.listdir(os.path.join(FLOWER_DATASET_DIRECTORY, 'flower_photos'))\n",
    "        if 'daisy' in flower_photos_listing and 'roses' in flower_photos_listing and 'dandelion' in flower_photos_listing and 'sunflowers' in flower_photos_listing and 'tulips' in flower_photos_listing:\n",
    "            in_unpacked = True\n",
    "\n",
    "if not is_unpacked:\n",
    "    if not is_downloaded:\n",
    "        download_file(FLOWER_CLASSIFICATION_URL, os.path.join(FLOWER_DATASET_DIRECTORY, FLOWER_DATASET_ARCHIVE_FILENAME))\n",
    "    extract_flower_archive(os.path.join(FLOWER_DATASET_DIRECTORY, FLOWER_DATASET_ARCHIVE_FILENAME), FLOWER_DATASET_DIRECTORY)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b93e20-e4ed-487b-827b-edf5fd9a60d9",
   "metadata": {},
   "source": [
    "### Create the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9d8bb7a-4d78-49cb-a4db-c2db320b02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for file in os.listdir(os.path.join(FLOWER_DATASET_DIRECTORY, 'flower_photos')):\n",
    "    class_dir = os.path.join(FLOWER_DATASET_DIRECTORY, 'flower_photos', file)\n",
    "    if os.path.isdir(class_dir):\n",
    "        new_class = []\n",
    "        for image in os.listdir(class_dir):\n",
    "            new_class.append(os.path.join(class_dir, image))\n",
    "        classes.append(new_class)\n",
    "\n",
    "dataset = []\n",
    "for i, cl in enumerate(classes):\n",
    "    for sample in enumerate(cl):\n",
    "        dataset.append({ 'class': i, 'path': sample })\n",
    "\n",
    "train_paths, validate_paths, test_paths = random_split(dataset, [TRAIN_DATASET_RATIO, VAL_DATASET_RATIO, TEST_DATASET_RATIO])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b588bf14-7d92-4c8d-9d7e-dfc3a74d895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClassificationDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        image = read_image(self.samples[idx]['path'][1])\n",
    "        \n",
    "\n",
    "        image = image.float() / 255\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.samples[idx]['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e49984-3cd1-43ae-9edb-b91c96075337",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Resize(size=(224, 224))\n",
    "\n",
    "train_dataset = FlowerClassificationDataset(train_paths, transforms)\n",
    "val_dataset = FlowerClassificationDataset(validate_paths, transforms)\n",
    "test_dataset = FlowerClassificationDataset(test_paths, transforms)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=5)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=5)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe18adc-8a37-432b-a45e-a633395a216a",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e7db8c2-47fd-47f6-b457-201aaee014ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint version 46!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/macron/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/macron/.pyenv/versions/3.11.6/lib/python3.11/site-packages/lightning/pytorch/core/saving.py:177: Found keys that are not in the model state dict but in the checkpoint: ['model.fc.weight', 'model.fc.bias']\n"
     ]
    }
   ],
   "source": [
    "class MuseumCC0Module(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_module = torch.nn.CrossEntropyLoss()\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', weights=None)\n",
    "        self.out_features = self.model.fc.in_features\n",
    "        self.model.fc = torch.nn.Identity()\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        return self.model(imgs)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=0.1)   \n",
    "\n",
    "\n",
    "def get_newest_model():\n",
    "    logs_paths = os.path.join(CHECKPOINTS_DIRECTORY, 'lightning_logs')\n",
    "    newest_version = -1\n",
    "    checkpoint_path = None\n",
    "    for dir in os.listdir(logs_paths):\n",
    "        dir_path = os.path.join(logs_paths, dir)\n",
    "        if os.path.isdir(dir_path) and 'version_' in dir:\n",
    "            num = int(dir.replace('version_', ''))\n",
    "            if newest_version > num:\n",
    "                continue\n",
    "            \n",
    "            listing = os.listdir(dir_path)\n",
    "            if 'checkpoints' in listing:\n",
    "                check_dir = os.path.join(dir_path, 'checkpoints')\n",
    "                for filename in os.listdir(check_dir):\n",
    "                    if '.ckpt' in filename:\n",
    "                        checkpoint_path = os.path.join(check_dir, filename)\n",
    "                        newest_version = num\n",
    "\n",
    "    print(f'Using checkpoint version {newest_version}!') \n",
    "    return checkpoint_path\n",
    "\n",
    "if USE_RANDOM_MODEL:\n",
    "    base_model = MuseumCC0Module()\n",
    "else:\n",
    "    base_model = MuseumCC0Module.load_from_checkpoint(get_newest_model(), strict=False)\n",
    "in_features = base_model.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6628861a-bb34-47fc-b6fe-2da13c587921",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FlowerClassificationModule(LightningModule):\n",
    "    def __init__(self, model, in_features):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.out = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(64, 5),\n",
    "        )\n",
    "        self.loss_module = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        x = self.model(imgs)\n",
    "        return self.out(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.out.parameters(), lr=LEARNING_RATE)   \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "\n",
    "        x = self.model(images)\n",
    "        preds = self.out(x)\n",
    "        \n",
    "        loss = self.loss_module(preds, labels)\n",
    "\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "\n",
    "        self.log(\"train_acc\", acc, on_step=True, on_epoch=True)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        return loss \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        \n",
    "        x = self.model(images)\n",
    "        preds = self.out(x)\n",
    "\n",
    "        loss = self.loss_module(preds, labels)\n",
    "\n",
    "        acc = (labels == preds.argmax(dim=-1)).float().mean()\n",
    "\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        \n",
    "        x = self.model(images)\n",
    "        preds = self.out(x)\n",
    "\n",
    "        loss = self.loss_module(preds, labels)\n",
    "\n",
    "        acc = (labels == preds.argmax(dim=-1)).float().mean()\n",
    "\n",
    "        self.log(\"test_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "model = FlowerClassificationModule(base_model, in_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b687c18-8c90-413b-afb5-0ba5b6993f88",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "008be1b4-0363-441b-999b-0a69fc202960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | model       | MuseumCC0Module  | 23.5 M\n",
      "1 | out         | Sequential       | 1.2 M \n",
      "2 | loss_module | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "23.5 M    Non-trainable params\n",
      "24.7 M    Total params\n",
      "98.920    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198db69eee5c4aa596f72e4aaa39e44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(default_root_dir=FLOWER_CHECKPOINTS_DIRECTORY, max_epochs=EPOCHS)\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bedee6-6d09-4055-8eb1-1e9bab12636a",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "The ImageNet-trained has managed to get loss of 0.3605 and accuracy of 88.71%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad1c7313-3e65-4aed-9ccb-349ba5bb39c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macron/.pyenv/versions/3.11.6/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.\n",
      "Restoring states from the checkpoint path at flowers-checkpoints/lightning_logs/version_36/checkpoints/epoch=9-step=810.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at flowers-checkpoints/lightning_logs/version_36/checkpoints/epoch=9-step=810.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ba0f7c4182484b9d8418ce9d508dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3351498544216156     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.5038419961929321     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3351498544216156    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.5038419961929321    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.3351498544216156, 'test_loss': 1.5038419961929321}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ad5d8-c0a7-4a91-886f-0648436e84cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
