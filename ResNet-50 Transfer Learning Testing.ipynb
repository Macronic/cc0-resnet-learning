{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d4f4620-fcd6-4d20-818d-170612cb94e4",
   "metadata": {},
   "source": [
    "# ResNet-50 Transfer Learning Testing\n",
    "In this notebook, we'll be loading the last trained checkpoint and comparing it to ResNet-50 trained on ImageNet, using Flower Classification CC BY 4.0 dataset, [available here](https://data.mendeley.com/datasets/738sdjm6h9/1).\n",
    "\n",
    "We try to use the same hyperparameters as [this notebook](https://github.com/ovh/ai-training-examples/blob/main/notebooks/computer-vision/image-classification/tensorflow/resnet50/notebook-resnet-transfer-learning-image-classification.ipynb), so that we're able to compare it to a ResNet-50 trained on ImageNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dfe3bc-cf35-4601-a5a7-f1a799aa79cb",
   "metadata": {},
   "source": [
    "### Constants and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3451223-aadc-4855-830d-05093b9b5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import requests\n",
    "import tarfile\n",
    "from tqdm.notebook import tqdm\n",
    "from lightning.pytorch import Trainer, seed_everything, LightningModule\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import utils\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "CHECKPOINTS_DIRECTORY = 'checkpoints'\n",
    "FLOWER_DATASET_DIRECTORY = 'flowers'\n",
    "FLOWER_DATASET_ARCHIVE_FILENAME = 'flower_photos.tgz'\n",
    "\n",
    "# Notebook and Keras Adam default values\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "EPSILON = 1e-7\n",
    "\n",
    "TRAIN_DATASET_RATIO = 0.7\n",
    "VAL_DATASET_RATIO = 0.2\n",
    "TEST_DATASET_RATIO = 0.1\n",
    "\n",
    "FLOWER_CLASSIFICATION_URL = 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95ae65-18a2-40ce-b1b0-170a303f960f",
   "metadata": {},
   "source": [
    "### Download Flower Classification dataset if it isn't downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e19232d-8a7c-4ff4-92e1-cb05a51a7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(FLOWER_DATASET_DIRECTORY):\n",
    "    os.makedirs(FLOWER_DATASET_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46cdc421-0f84-49ad-88c1-6ef8b466a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_downloaded = False\n",
    "is_unpacked = False\n",
    "\n",
    "def download_file(url, path):\n",
    "    image_data = requests.get(url, stream=True)\n",
    "    if image_data.status_code == 200:\n",
    "        with open(path, 'wb') as f:\n",
    "            for chunk in image_data.iter_content(2048):\n",
    "                f.write(chunk)\n",
    "\n",
    "def extract_flower_archive(archive_path, target_path):\n",
    "    tar = tarfile.open(archive_path, 'r')\n",
    "    for file in tar.getmembers():\n",
    "        if file.isdir() or '.jpg' in file.name:\n",
    "            tar.extract(file, target_path)\n",
    "\n",
    "flower_directory_listing = os.listdir(FLOWER_DATASET_DIRECTORY)\n",
    "if len(flower_directory_listing) != 0:\n",
    "    if 'flower_photos.tgz' in flower_directory_listing:\n",
    "        is_downloaded = True\n",
    "    if 'flower_photos' in flower_directory_listing:\n",
    "        flower_photos_listing = os.listdir(os.path.join(FLOWER_DATASET_DIRECTORY, flower_photos))\n",
    "        if 'daisy' in flower_photos_listing and 'roses' in flower_photos_listing and 'dandelion' in flower_photos_listing and 'sunflowers' in flower_photos_listing and 'tulips' in flower_photos_listing:\n",
    "            in_unpacked = True\n",
    "\n",
    "if not is_unpacked:\n",
    "    if not is_downloaded:\n",
    "        download_file(FLOWER_CLASSIFICATION_URL, os.path.join(FLOWER_DATASET_DIRECTORY, FLOWER_DATASET_ARCHIVE_FILENAME))\n",
    "    extract_flower_archive(os.path.join(FLOWER_DATASET_DIRECTORY, FLOWER_DATASET_ARCHIVE_FILENAME), FLOWER_DATASET_DIRECTORY)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b93e20-e4ed-487b-827b-edf5fd9a60d9",
   "metadata": {},
   "source": [
    "### Create the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9d8bb7a-4d78-49cb-a4db-c2db320b02da",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "for file in os.listdir(os.path.join(FLOWER_DATASET_DIRECTORY, 'flower_photos')):\n",
    "    if os.path.isdir(os.path.join(FLOWER_DATASET_DIRECTORY, 'flower_photos', file)):\n",
    "        new_class = []\n",
    "        for image in os.listdir(os.path.join(FLOWER_DATASET_DIRECTORY, 'flower_photos', file)):\n",
    "            new_class.append(image)\n",
    "        classes.append(new_class)\n",
    "\n",
    "dataset = []\n",
    "for i, cl in enumerate(classes):\n",
    "    for sample in enumerate(cl):\n",
    "        dataset.append({ 'class': i, 'path': sample })\n",
    "\n",
    "train_paths, validate_paths, test_paths = random_split(dataset, [TRAIN_DATASET_RATIO, VAL_DATASET_RATIO, TEST_DATASET_RATIO])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b588bf14-7d92-4c8d-9d7e-dfc3a74d895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowerClassificationDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = read_image(self.samples[idx]['path'])\n",
    "\n",
    "        image = image.float() / 255\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.samples[idx]['classes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9e49984-3cd1-43ae-9edb-b91c96075337",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Resize(size=(224, 224))\n",
    "\n",
    "train_dataset = FlowerClassificationDataset(train_paths, transforms)\n",
    "val_dataset = FlowerClassificationDataset(validate_paths, transforms)\n",
    "test_dataset = FlowerClassificationDataset(test_paths, transforms)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=5)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=5)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe18adc-8a37-432b-a45e-a633395a216a",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7db8c2-47fd-47f6-b457-201aaee014ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00b4e1f6-e96a-431d-86cd-f37dc882a5ae",
   "metadata": {},
   "source": [
    "### Change layers of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb6136-4da9-4871-8d0a-87dd38660902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b687c18-8c90-413b-afb5-0ba5b6993f88",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008be1b4-0363-441b-999b-0a69fc202960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0bedee6-6d09-4055-8eb1-1e9bab12636a",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "The ImageNet-trained has managed to get loss of 0.3605 and accuracy of 88.71%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1c7313-3e65-4aed-9ccb-349ba5bb39c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
